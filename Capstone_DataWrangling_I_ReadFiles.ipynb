{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as opath\n",
    "import ast\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Note book details</h3>\n",
    "\n",
    "<p> This notebook is part 1 of 2 for data wrangling for <b>kickstarter success prediction</b> project.</p>\n",
    "\n",
    "<p> Notes.</p>\n",
    "\n",
    "<ol>\n",
    "<li>  The file read the multiple csv files as input for each month </li> \n",
    "<li>  The file needs to be run manually for each month as files for following reason \n",
    "    <ul>\n",
    "     <li> The files are available at end of months</li> \n",
    "    <li>  Processing each month is time consuming and resource extensive process on local computer </li> \n",
    "    </ul>\n",
    "</li> \n",
    "<li>  The files are pickeld and csv to use further in project . This make easier to use data in later stages</li> \n",
    "    \n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading all files in monthly folder\n",
    "df_month =pd.DataFrame()\n",
    "folderpath='data/Kickstarter_2018-01-12T10_20_09_196Z'\n",
    "picklefilename='df_Jan18_f1.sa'\n",
    "csvfilename='df_Jan18_f1.csv'\n",
    "\n",
    "for root,_,files in os.walk(folderpath):\n",
    "    for f in files:\n",
    "        fullpath=opath.join(root,f)\n",
    "        df_file=pd.read_csv(fullpath)\n",
    "        df_month= df_month.append(df_file,ignore_index=True)\n",
    "df_month.set_index('id').sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling and extracting values from sub values in column. The values are as Json object which needed to be parsed.\n",
    "\n",
    "### This step is core of extracting data . Hence  This step runs for few hours on local machine hence some values printed in steps to confirm if process is still running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting coulmns needed to get the values\n",
    "f=f[['id','name',\n",
    "           'category',\n",
    "           'goal','pledged','state','staff_pick','spotlight','backers_count','country','currency','usd_pledged',\n",
    "           'deadline','state_changed_at','created_at','launched_at',\n",
    "           'creator',# name of creator\n",
    "           'location',# city \n",
    "           'urls',#urlto browse\n",
    "           'disable_communication','slug']]#.head(10)\n",
    "df_userinfo = pd.DataFrame()\n",
    "\n",
    "# iterating over all rows to extract data\n",
    "for index, row in f.iterrows():\n",
    "   # Json normalizing the row to get namd and unique id for the project \n",
    "    uinfo=(json_normalize(json.loads(row.iloc[16]),record_path ='urls', meta=['name','id']))#, meta=['name','is_registered', 'id']))\n",
    "    uinfo['projectid']=row.id#f.iloc[index].id\n",
    "    \n",
    "    # getting category value id \n",
    "    uinfo['Catgegory_val']=pd.read_json((row.iloc[2])).iloc[0][2]\n",
    "    \n",
    "    # getting deadline value\n",
    "    uinfo['deadline_val']=( datetime.datetime.fromtimestamp(\n",
    "        int(row['deadline'] )\n",
    "    ).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    uinfo['deadline_val']=( datetime.datetime.fromtimestamp(\n",
    "        int(row['deadline'] )\n",
    "    ).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # getting state change value\n",
    "    uinfo['state_changed_at_val']=( datetime.datetime.fromtimestamp(\n",
    "        int(row['state_changed_at'] )\n",
    "    ).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # getting created at value\n",
    "    uinfo['created_at_val']=( datetime.datetime.fromtimestamp(\n",
    "        int(row['created_at'] )\n",
    "    ).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # getting launched at value\n",
    "    uinfo['launched_at_val']=( datetime.datetime.fromtimestamp(\n",
    "        int(row['launched_at'] )\n",
    "    ).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # print to check process running\n",
    "    print(row.id)\n",
    "    print(index)\n",
    "    # getting location related value ( city, state type)\n",
    "    if(str(row.iloc[17])!='nan'):\n",
    "        linfo=pd.read_json(str(row.iloc[17]),encoding='UTF-8')\n",
    "        uinfo['location_cityname']=linfo.iloc[0][4]\n",
    "        uinfo['location_state']=linfo.iloc[0][7]\n",
    "        uinfo['location_type']=linfo.iloc[0][8]\n",
    "    else:\n",
    "        uinfo['location_cityname']=np.NaN\n",
    "        uinfo['location_state']=np.NaN\n",
    "        uinfo['location_type']=np.NaN\n",
    "    \n",
    "    # getting project url for kickstarter with intention of web scrappinn information if required\n",
    "    uinfo['Project_url']=(pd.read_json((row.iloc[18]))).iloc[0][0]\n",
    "    \n",
    "    # print to check end of processing of one row\n",
    "    df_userinfo=df_userinfo.append(uinfo.iloc[1:,1:],ignore_index=True)\n",
    "    print('**************************')\n",
    "\n",
    "# Renaming coulms to more meaningful names\n",
    "df_userinfo.rename(columns={'name': 'username', 'id': 'userid','projectid':'id'}, inplace=True)\n",
    "f1=f.merge(df_userinfo,on='id')\n",
    "\n",
    "#f1#.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle and save the single monthly file for further processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the pickel file name to re confirm the previous files are not overriden.\n",
    "# Note: Run this file after confimring succesful completion of all above steps\n",
    "print(picklefilename)\n",
    "f1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create pickle file for further use \n",
    "pickle.dump(f1,open(picklefilename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create csv file for further use \n",
    "f1.to_csv(csvfilename,na_rep='NaN',encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check pickel file\n",
    "upickle_df = pickle.load(open(picklefilename,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upickle_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upickle_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
